import os
import sys
import warnings

from sklearn.exceptions import UndefinedMetricWarning

from src.data_loader import load_veracity
from src.models import hmm_veracity, veracity_majority_baseline
import random
import copy

current_path = os.path.abspath(__file__)
pheme_data_path = os.path.join(current_path, '../../../data/datasets/pheme/preprocessed/veracity/')
dast_data_path = os.path.join(current_path, '../../../data/datasets/dast/preprocessed/veracity/')


class HiddenPrints:
    """Class for suppressing the abundant print statements from hmm_veracity.test, courtesy of
    https://stackoverflow.com/questions/8391411/suppress-calls-to-print-python"""
    def __enter__(self):
        self._original_stdout = sys.stdout
        sys.stdout = open(os.devnull, 'w')
        warnings.filterwarnings("ignore", category=UndefinedMetricWarning)
        warnings.filterwarnings("ignore", category=RuntimeWarning)

    def __exit__(self, exc_type, exc_val, exc_tb):
        sys.stdout.close()
        sys.stdout = self._original_stdout


def format_data(pheme_ts, pheme_nts, dast_ts, dast_nts, testdata_type, xv_count):
    if testdata_type == 'dast':
        dast_ts = cross_validation_splits(dast_ts, xv_count)
        dast_nts = cross_validation_splits(dast_nts, xv_count)

    if testdata_type == 'pheme':
        pheme_ts = cross_validation_splits(pheme_ts, xv_count)
        pheme_nts = cross_validation_splits(pheme_nts, xv_count)

    if testdata_type == 'dastpheme':
        pheme_ts = cross_validation_splits(pheme_ts, xv_count)
        pheme_nts = cross_validation_splits(pheme_nts, xv_count)
        dast_ts = cross_validation_splits(dast_ts, xv_count)
        dast_nts = cross_validation_splits(dast_nts, xv_count)

    return pheme_ts, pheme_nts, dast_ts, dast_nts


def load_datasets(unverified_cast, testdata_type, remove_commenting, xv_count):
    """Loads the relevant datasets used for testing of veracity performance"""
    if testdata_type not in ['dast', 'pheme', 'dastpheme']:
        err_msg = "Unrecognized test_dataset type {}, please use 'dast' or 'dastpheme'"
        raise RuntimeError(
            err_msg.format(testdata_type))

    pheme_ts = load_veracity(os.path.join(pheme_data_path, 'timestamps.csv'), unverified_cast, remove_commenting)
    pheme_nts = load_veracity(os.path.join(pheme_data_path, 'no_timestamps.csv'), unverified_cast, remove_commenting)

    dast_ts = load_veracity(os.path.join(dast_data_path, 'timestamps.csv'), unverified_cast, remove_commenting)
    dast_nts = load_veracity(os.path.join(dast_data_path, 'no_timestamps.csv'), unverified_cast, remove_commenting)
    return format_data(pheme_ts, pheme_nts, dast_ts, dast_nts, testdata_type, xv_count)


def cross_validation_splits(dataset, no_splits):
    """Generate an array of dataset splits, each index containing a tuple with train data at 0 and test data at 1. Takes
    preprocessed veracity data, as generated by data_loader.load_veracity() as input.

    :param dataset: an array which, at each index, contains a tuple, with the veracity of a claim at 0 and an array
    containing the features at index 1
    :param no_splits: The number of partitions the dataset should be split into
    :return: an array of dimensions (no_splits x 2 (train data at 0 and test data at 1) x len(train) or len(test))
    """
    splits = []
    random.shuffle(dataset)

    # A few data-points might be lost due to rounding
    splitsize = int(len(dataset)/no_splits)
    last_loaded_index = 0
    for i in range(no_splits):
        test_data = dataset[last_loaded_index:last_loaded_index+splitsize]
        train_data = dataset[:last_loaded_index]
        train_data.extend(dataset[last_loaded_index+splitsize:])
        splits.append((train_data, test_data))
        last_loaded_index += splitsize
    return splits


def update_metrics(performance, model, f1_macro, f1_micro, acc, precision, recall, branch_length):
    if branch_length:
        performance[model][branch_length]['f1_macro'] += f1_macro
        performance[model][branch_length]['f1_micro'] += f1_micro
        performance[model][branch_length]['accuracy'] += acc
        performance[model][branch_length]['precision'] += precision
        performance[model][branch_length]['recall'] += recall


    else:
        performance[model]['f1_macro'] += f1_macro
        performance[model]['f1_micro'] += f1_micro
        performance[model]['accuracy'] += acc
        performance[model]['precision'] += precision
        performance[model]['recall'] += recall


def test_setup(pheme_data, dast_data, testdata_type, empty_performance, model_type, comp_count):
    performance = {x: copy.deepcopy(empty_performance) for x in ['pheme', 'dastpheme', 'dast', 'dast_majority',
                                                                 'pheme_majority', 'dastpheme_majority']}
    models = {}

    if testdata_type == 'pheme':
        models['dast'] = hmm_veracity.HMM(comp_count, model_type).fit(dast_data)
        models['dast_majority'] = veracity_majority_baseline.VeracityMajorityBaseline().fit(dast_data)

    if testdata_type == 'dast':
        models['pheme'] = hmm_veracity.HMM(comp_count, model_type).fit(pheme_data)
        models['pheme_majority'] = veracity_majority_baseline.VeracityMajorityBaseline().fit(pheme_data)
    return performance, models


def split_setup(split_index, testdata_type, dast_data, pheme_data, models, model_type, comp_count):
    if testdata_type == 'dast':
        test_data = dast_data[split_index][1]
        models['dast'] = hmm_veracity.HMM(comp_count, model_type).fit(dast_data[split_index][0])
        models['dastpheme'] = hmm_veracity.HMM(comp_count, model_type).fit(pheme_data + dast_data[split_index][0])
        models['dast_majority'] = veracity_majority_baseline.VeracityMajorityBaseline().fit(dast_data[split_index][0])
        models['dastpheme_majority'] = veracity_majority_baseline.VeracityMajorityBaseline().fit(pheme_data + dast_data[split_index][0])

    if testdata_type == 'pheme':
        test_data = pheme_data[split_index][1]
        models['pheme'] = hmm_veracity.HMM(comp_count, model_type).fit(pheme_data[split_index][0])
        models['dastpheme'] = hmm_veracity.HMM(comp_count, model_type).fit(dast_data + pheme_data[split_index][0])
        models['pheme_majority'] = veracity_majority_baseline.VeracityMajorityBaseline().fit(pheme_data[split_index][0])
        models['dastpheme_majority'] = veracity_majority_baseline.VeracityMajorityBaseline().fit(dast_data + pheme_data[split_index][0])

    if testdata_type == 'dastpheme':
        test_data = dast_data[split_index][1] + pheme_data[split_index][1]
        models['dast'] = hmm_veracity.HMM(2, model_type).fit(dast_data[split_index][0])
        models['pheme'] = hmm_veracity.HMM(2, model_type).fit(pheme_data[split_index][0])
        models['dastpheme'] = hmm_veracity.HMM(2, model_type).fit(pheme_data[split_index][0] + dast_data[split_index][0])
        models['dast_majority'] = veracity_majority_baseline.VeracityMajorityBaseline().fit(dast_data[split_index][0])
        models['pheme_majority'] = veracity_majority_baseline.VeracityMajorityBaseline().fit(pheme_data[split_index][0])
        models['dastpheme_majority'] = veracity_majority_baseline.VeracityMajorityBaseline().fit(pheme_data[split_index][0] + dast_data[split_index][0])
    return models, test_data


def evaluate_for_splits_dataset(pheme_data, dast_data, unverified_cast, testdata_type, model_type, xv_count, comp_count):
    empty_performance = {'f1_macro': 0.0, 'f1_micro': 0.0, 'accuracy': 0.0, 'precision': 0.0, 'recall': 0.0}
    performance, models = test_setup(pheme_data, dast_data, testdata_type, empty_performance, model_type, comp_count)

    for i in range(xv_count):
        models, test_data = split_setup(i, testdata_type, dast_data, pheme_data, models, model_type, comp_count)

        for model_name, model in models.items():
            _, acc, f1_macro, f1_micro, precision, recall = model.test(test_data, unverified_cast)
            update_metrics(performance, model_name, f1_macro, f1_micro, acc, precision, recall, branch_length=False)

    for dataset, results in performance.items():
        for metric, value in results.items():
            performance[dataset][metric] = value / xv_count

    models.clear()
    return performance


def evaluate_for_splits_length(pheme_data, dast_data, unverified_cast, testdata_type, model_type, xv_count, comp_count):
    empty_performance = {x: {'f1_macro': 0.0, 'f1_micro': 0.0, 'accuracy': 0.0, 'precision': 0.0, 'recall': 0.0} for x in [1, 2, 3, 4, 6, 8, 10]}
    performance, models = test_setup(pheme_data, dast_data, testdata_type, empty_performance, model_type, comp_count)

    for i in range(xv_count):
        length_separated_data = {1: [], 2: [], 3: [], 4: [], 6: [], 8: [], 10: []}

        models, test_data = split_setup(i, testdata_type, dast_data, pheme_data, models, model_type, comp_count)

        for branch in test_data:
            if len(branch[1]) in length_separated_data:
                length_separated_data[len(branch[1])].append(branch)
            elif len(branch[1]) == 5:
                length_separated_data[4].append(branch)
            elif len(branch[1]) == 7:
                length_separated_data[6].append(branch)
            elif len(branch[1]) == 9:
                length_separated_data[8].append(branch)
            elif len(branch[1]) >= 10:
                length_separated_data[10].append(branch)

        for length, datapoints in length_separated_data.items():
            if datapoints:
                for model_name, model in models.items():
                    _, acc, f1_macro, f1_micro, precision, recall = model.test(datapoints, unverified_cast)
                    update_metrics(performance, model_name, f1_macro, f1_micro, acc, precision, recall, branch_length=length)

    for dataset, results in performance.items():
        for length, metrics in results.items():
            for metric, value in metrics.items():
                performance[dataset][length][metric] = value / xv_count
    models.clear()
    return performance


def write_out(unverified_cast, remove_commenting, testdata_type, model_type, include_branch_length, comp_count, performance, out_path='veracity.csv'):
    empty_header = False
    if os.path.getsize(out_path) == 0:
        empty_header = True

    with open(out_path, mode='a', encoding='utf-8') as out_file:
        if include_branch_length:
            if empty_header:
                out_file.write('model;length;unverified_cast;remove_commenting;testdata_type;model_type;comp_count;f1_macro;f1_micro;accuracy;precision;recall\n')
            for dataset, results in performance[0].items():
                for length, metrics in results.items():
                    out_file.write('{};{};{};{};{};{};{};{:.3f};{:.3f};{:.3f};{:.3f};{:.3f}\n'.format(dataset, length, unverified_cast, remove_commenting, testdata_type, model_type, comp_count, results[length]['f1_macro'], results[length]['f1_micro'], results[length]['accuracy'], results[length]['precision'], results[length]['recall']))
            if len(performance) > 1:
                for dataset, results in performance[1].items():
                    for length, metrics in results.items():
                        if 'majority' in dataset:
                            continue
                        out_file.write('{};{};{};{};{};{};{};{:.3f};{:.3f};{:.3f};{:.3f};{:.3f}\n'.format(dataset + '_ts', length, unverified_cast, remove_commenting, testdata_type, model_type, comp_count, results[length]['f1_macro'], results[length]['f1_micro'], results[length]['accuracy'], results[length]['precision'], results[length]['recall']))

        else:
            if empty_header:
                out_file.write('model;unverified_cast;remove_commenting;testdata_type;model_type;comp_count;f1_macro;f1_micro;accuracy;precision;recall\n')
            for dataset, results in performance[0].items():
                out_file.write('{};{};{};{};{};{};{:.3f};{:.3f};{:.3f};{:.3f};{:.3f}\n'.format(dataset, unverified_cast, remove_commenting, testdata_type, model_type, comp_count, results['f1_macro'], results['f1_micro'], results['accuracy'], results['precision'], results['recall']))
            if len(performance) > 1:
                for dataset, results in performance[1].items():
                    if 'majority' in dataset:
                        continue
                    out_file.write(
                        '{};{};{};{};{};{};{:.3f};{:.3f};{:.3f};{:.3f};{:.3f}\n'.format(dataset + '_ts', unverified_cast, remove_commenting, testdata_type, model_type, comp_count, results['f1_macro'], results['f1_micro'], results['accuracy'], results['precision'], results['recall']))


def evaluate_performance(unverified_cast, remove_commenting, testdata_type='dast', model_type='gaussian', include_branch_length=False, xv_count=5, comp_count=2):
    pheme_ts, pheme_nts, dast_ts, dast_nts = load_datasets(unverified_cast, testdata_type, remove_commenting, xv_count)
    performance = []
    if include_branch_length:
        nts_performance = evaluate_for_splits_length(pheme_nts, dast_nts, unverified_cast, testdata_type, model_type, xv_count, comp_count)
        performance.append(nts_performance)
        if model_type in ['gaussian']:
            ts_performance = evaluate_for_splits_length(pheme_ts, dast_ts, unverified_cast, testdata_type, model_type, xv_count, comp_count)
            performance.append(ts_performance)
    else:
        nts_performance = evaluate_for_splits_dataset(pheme_nts, dast_nts, unverified_cast, testdata_type, model_type, xv_count, comp_count)
        performance.append(nts_performance)
        if model_type in ['gaussian']:
            ts_performance = evaluate_for_splits_dataset(pheme_ts, dast_ts, unverified_cast, testdata_type, model_type, xv_count, comp_count)
            performance.append(ts_performance)
    write_out(unverified_cast, remove_commenting, testdata_type, model_type, include_branch_length, comp_count, performance)
    performance.clear(), pheme_ts.clear(), dast_ts.clear(), dast_nts.clear()


def full_performance_eval(include_branch_length=False, fix_comp_count=True, xv_count=5):
    unverified_cast_vars = ['true', 'false']
    remove_commenting_vars = ['true', 'false']
    testdata_type_vars = ['dast', 'pheme', 'dastpheme']
    model_type_vars = ['gaussian', 'multinomial']
    comp_count_vars = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
    for unverified_cast in unverified_cast_vars:
        for remove_commenting in remove_commenting_vars:
            for testdata_type in testdata_type_vars:
                for model_type in model_type_vars:
                    if not fix_comp_count:
                        for comp_count in comp_count_vars:
                            with HiddenPrints():
                                evaluate_performance(unverified_cast, remove_commenting, testdata_type, model_type, include_branch_length, xv_count, comp_count)
                                print('Performance eval done for: unverified_cast: {}, remove_commenting: {}, testdata_type: {}, model_type: {}, comp_count: {}'.format(
                                    unverified_cast, remove_commenting, testdata_type, model_type, comp_count
                                ))
                    else:
                        with HiddenPrints():
                            evaluate_performance(unverified_cast, remove_commenting, testdata_type, model_type, include_branch_length, xv_count)
                            print('Performance eval done for: unverified_cast: {}, remove_commenting: {}, testdata_type: {}, model_type: {}'.format(
                                unverified_cast, remove_commenting, testdata_type, model_type
                            ))


full_performance_eval(include_branch_length=False)
