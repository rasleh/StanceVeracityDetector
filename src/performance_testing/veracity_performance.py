import os
import sys

from src.data_loader import load_veracity
from src.models import hmm_veracity
import random

current_path = os.path.abspath(__file__)
pheme_data_path = os.path.join(current_path, '../../../data/datasets/pheme/preprocessed/veracity/')
dast_data_path = os.path.join(current_path, '../../../data/datasets/dast/preprocessed/veracity/')


class HiddenPrints:
    """Class for suppressing the abundant print statements from hmm_veracity.test, courtesy of
    https://stackoverflow.com/questions/8391411/suppress-calls-to-print-python"""
    def __enter__(self):
        self._original_stdout = sys.stdout
        sys.stdout = open(os.devnull, 'w')

    def __exit__(self, exc_type, exc_val, exc_tb):
        sys.stdout.close()
        sys.stdout = self._original_stdout


def cross_validation_splits(dataset, no_splits):
    """Generate an array of dataset splits, each index containing a tuple with train data at 0 and test data at 1. Takes
    preprocessed veracity data, as generated by data_loader.load_veracity() as input.

    :param dataset: an array which, at each index, contains a tuple, with the veracity of a claim at 0 and an array
    containing the features at index 1
    :param no_splits: The number of partitions the dataset should be split into
    :return: an array of dimensions (no_splits x 2 (representing train and test data) x len(train) or len(test)
    """
    splits = []

    # A few data-points will be lost due to rounding, 3 for DAST
    splitsize = int(len(dataset)/no_splits)
    last_loaded_index = 0
    for i in range(no_splits):
        test_data = dataset[last_loaded_index:last_loaded_index+splitsize]
        train_data = dataset[:last_loaded_index]
        train_data.extend(dataset[last_loaded_index+splitsize:])
        splits.append((train_data, test_data))
        last_loaded_index += splitsize
    return splits


def load_datasets(unverified_cast):
    """Loads the relevant datasets used for testing of veracity performance"""
    pheme_ts = load_veracity(os.path.join(pheme_data_path, 'timestamps.csv'), unverified_cast)
    pheme_nts = load_veracity(os.path.join(pheme_data_path, 'no_timestamps.csv'), unverified_cast)

    full_dast_ts = load_veracity(os.path.join(dast_data_path, 'timestamps.csv'), unverified_cast)
    full_dast_nts = load_veracity(os.path.join(dast_data_path, 'no_timestamps.csv'), unverified_cast)

    random.shuffle(full_dast_ts)
    random.shuffle(full_dast_nts)

    dast_ts_splits = cross_validation_splits(full_dast_ts, 5)
    dast_nts_splits = cross_validation_splits(full_dast_nts, 5)
    return pheme_ts, pheme_nts, dast_ts_splits, dast_nts_splits


def update_metrics(performance, model, acc, f1):
    performance[model]['f1_macro'] += f1
    performance[model]['accuracy'] += acc


def evaluate_for_splits(pheme_data, dast_data, unverified_cast):
    pheme_model = hmm_veracity.HMM(2)
    pheme_model.fit(pheme_data)

    performance = {
        'dast': {'f1_macro': 0.0, 'accuracy': 0.0},
        'pheme': {'f1_macro': 0.0, 'accuracy': 0.0},
        'dastpheme': {'f1_macro': 0.0, 'accuracy': 0.0}
    }

    for split in dast_data:
        _, acc, f1_macro, _ = pheme_model.test(split[1], unverified_cast)
        update_metrics(performance, 'pheme', acc, f1_macro)

        dastpheme_model = pheme_model
        dastpheme_model.fit(split[0])
        _, acc, f1_macro, _ = dastpheme_model.test(split[1], unverified_cast)
        update_metrics(performance, 'dastpheme', acc, f1_macro)

        dast_model = hmm_veracity.HMM(2)
        dast_model.fit(split[0])
        _, acc, f1_macro, _ = dast_model.test(split[1], unverified_cast)
        update_metrics(performance, 'dast', acc, f1_macro)

    for dataset, results in performance.items():
        for metric, value in results.items():
            performance[dataset][metric] = value / len(dast_data)

    return performance


def evaluate_performance(unverified_cast):
    pheme_ts, pheme_nts, dast_ts_splits, dast_nts_splits = load_datasets(unverified_cast)

    # Hide abundance of print statements for hmm_veracity.test()
    with HiddenPrints():
        ts_performance = evaluate_for_splits(pheme_ts, dast_ts_splits, unverified_cast)
        nts_performance = evaluate_for_splits(pheme_nts, dast_nts_splits, unverified_cast)

    with open('veracity.csv', mode='w', encoding='utf-8') as out_file:
        out_file.write('model;f1_macro;accuracy\n')
        for dataset, results in ts_performance.items():
            out_file.write('{};{:.2f};{:.2f}\n'.format(dataset+'_ts', results['f1_macro'], results['accuracy']))

        for dataset, results in nts_performance.items():
            out_file.write('{};{:.2f};{:.2f}\n'.format(dataset, results['f1_macro'], results['accuracy']))


evaluate_performance('true')
