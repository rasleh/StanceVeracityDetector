import os
import sys

from src.data_loader import load_veracity
from src.models import hmm_veracity
import random
import copy

current_path = os.path.abspath(__file__)
pheme_data_path = os.path.join(current_path, '../../../data/datasets/pheme/preprocessed/veracity/')
dast_data_path = os.path.join(current_path, '../../../data/datasets/dast/preprocessed/veracity/')


class HiddenPrints:
    """Class for suppressing the abundant print statements from hmm_veracity.test, courtesy of
    https://stackoverflow.com/questions/8391411/suppress-calls-to-print-python"""
    def __enter__(self):
        self._original_stdout = sys.stdout
        sys.stdout = open(os.devnull, 'w')

    def __exit__(self, exc_type, exc_val, exc_tb):
        sys.stdout.close()
        sys.stdout = self._original_stdout


def load_datasets(unverified_cast):
    """Loads the relevant datasets used for testing of veracity performance"""
    pheme_ts = load_veracity(os.path.join(pheme_data_path, 'timestamps.csv'), unverified_cast)
    pheme_nts = load_veracity(os.path.join(pheme_data_path, 'no_timestamps.csv'), unverified_cast)

    dast_ts = load_veracity(os.path.join(dast_data_path, 'timestamps.csv'), unverified_cast)
    dast_nts = load_veracity(os.path.join(dast_data_path, 'no_timestamps.csv'), unverified_cast)
    return pheme_ts, pheme_nts, dast_ts, dast_nts


def cross_validation_splits(dataset, no_splits):
    """Generate an array of dataset splits, each index containing a tuple with train data at 0 and test data at 1. Takes
    preprocessed veracity data, as generated by data_loader.load_veracity() as input.

    :param dataset: an array which, at each index, contains a tuple, with the veracity of a claim at 0 and an array
    containing the features at index 1
    :param no_splits: The number of partitions the dataset should be split into
    :return: an array of dimensions (no_splits x 2 (representing train and test data) x len(train) or len(test)
    """
    splits = []
    random.shuffle(dataset)

    # A few data-points might be lost due to rounding
    splitsize = int(len(dataset)/no_splits)
    last_loaded_index = 0
    for i in range(no_splits):
        test_data = dataset[last_loaded_index:last_loaded_index+splitsize]
        train_data = dataset[:last_loaded_index]
        train_data.extend(dataset[last_loaded_index+splitsize:])
        splits.append((train_data, test_data))
        last_loaded_index += splitsize
    return splits


def format_data(pheme_ts, pheme_nts, dast_ts, dast_nts, test_dataset):
    if test_dataset == 'dast':
        ts_data = (pheme_ts, cross_validation_splits(dast_ts, 5))
        nts_data = (pheme_nts, cross_validation_splits(dast_nts, 5))

    elif test_dataset == 'dastpheme':
        ts_data = cross_validation_splits(dast_ts + pheme_ts, 5)
        nts_data = cross_validation_splits(dast_nts + pheme_nts, 5)

    else:
        err_msg = "Unrecognized test_dataset type {}, please use 'dast' or 'dastpheme'"
        raise RuntimeError(
            err_msg.format(test_dataset))

    return ts_data, nts_data


def update_metrics(performance, model, acc, f1, branch_length):
    if branch_length:
        performance[model][branch_length]['f1_macro'] += f1
        performance[model][branch_length]['accuracy'] += acc
    else:
        performance[model]['f1_macro'] += f1
        performance[model]['accuracy'] += acc


def evaluate_for_splits_dataset_test(data, unverified_cast, test_dataset):
    performance = {}
    models = {}

    if test_dataset == 'dast':
        pheme_data = data[0]
        split_data = data[1]
        models['pheme'] = hmm_veracity.HMM(2).fit(pheme_data)

        performance['pheme'] = {'f1_macro': 0.0, 'accuracy': 0.0}
        performance['dastpheme'] = {'f1_macro': 0.0, 'accuracy': 0.0}
        performance['dast'] = {'f1_macro': 0.0, 'accuracy': 0.0}
    else:
        split_data = data

    for split in split_data:
        if test_dataset == 'dast':
            models['dastpheme'] = hmm_veracity.HMM(2).fit(pheme_data + split[0])
            models['dast'] = hmm_veracity.HMM(2).fit(split[0])

        for model_name, model in models.items():
            _, acc, f1_macro, _ = model.test(split[1], unverified_cast)
            update_metrics(performance, model_name, acc, f1_macro, branch_length=False)

    for dataset, results in performance.items():
        for metric, value in results.items():
            performance[dataset][metric] = value / len(split_data)

    return performance


def evaluate_for_splits_length(data, unverified_cast, test_dataset):
    performance = {}
    empty_performance = {
            1: {'f1_macro': 0.0, 'accuracy': 0.0},
            2: {'f1_macro': 0.0, 'accuracy': 0.0},
            3: {'f1_macro': 0.0, 'accuracy': 0.0},
            4: {'f1_macro': 0.0, 'accuracy': 0.0},
            6: {'f1_macro': 0.0, 'accuracy': 0.0},
            8: {'f1_macro': 0.0, 'accuracy': 0.0},
            10: {'f1_macro': 0.0, 'accuracy': 0.0},
        }

    models = {}

    if test_dataset == 'dast':
        pheme_data = data[0]
        split_data = data[1]
        models['pheme'] = hmm_veracity.HMM(2).fit(pheme_data)

        performance['pheme'] = copy.deepcopy(empty_performance)
        performance['dastpheme'] = copy.deepcopy(empty_performance)
        performance['dast'] = copy.deepcopy(empty_performance)
    else:
        split_data = data

    for split in split_data:
        length_separated_data = {1: [], 2: [], 3: [], 4: [], 6: [], 8: [], 10: []}
        for branch in split[1]:
            if len(branch[1]) in length_separated_data:
                length_separated_data[len(branch[1])].append(branch)
            elif len(branch[1]) == 5:
                length_separated_data[4].append(branch)
            elif len(branch[1]) == 7:
                length_separated_data[6].append(branch)
            elif len(branch[1]) == 9:
                length_separated_data[8].append(branch)
            elif len(branch[1]) >= 10:
                length_separated_data[10].append(branch)

        if test_dataset == 'dast':
            models['dastpheme'] = hmm_veracity.HMM(2).fit(pheme_data + split[0])
            models['dast'] = hmm_veracity.HMM(2).fit(split[0])

        for length, datapoints in length_separated_data.items():
            if datapoints:
                for model_name, model in models.items():
                    _, acc, f1_macro, _ = model.test(datapoints, unverified_cast)
                    update_metrics(performance, model_name, acc, f1_macro, branch_length=length)

    for dataset, results in performance.items():
        for length, metrics in results.items():
            for metric, value in metrics.items():
                performance[dataset][length][metric] = value / len(split_data)

    return performance


def write_out(include_branch_length, ts_performance, nts_performance, out_path='veracity.csv'):
    with open(out_path, mode='w', encoding='utf-8') as out_file:
        if include_branch_length:
            out_file.write('model;length;f1_macro;accuracy\n')
            for dataset, results in ts_performance.items():
                for length, metrics in results.items():
                    out_file.write('{};{};{:.2f};{:.2f}\n'.format(dataset + '_ts', length, results[length]['f1_macro'],
                                                                  results[length]['accuracy']))

            for dataset, results in nts_performance.items():
                for length, metrics in results.items():
                    out_file.write('{};{};{:.2f};{:.2f}\n'.format(dataset + '_nts', length, results[length]['f1_macro'],
                                                                  results[length]['accuracy']))

        else:
            out_file.write('model;f1_macro;accuracy\n')
            for dataset, results in ts_performance.items():
                out_file.write('{};{:.2f};{:.2f}\n'.format(dataset + '_ts', results['f1_macro'], results['accuracy']))

            for dataset, results in nts_performance.items():
                out_file.write('{};{:.2f};{:.2f}\n'.format(dataset, results['f1_macro'], results['accuracy']))


def evaluate_performance(unverified_cast, include_branch_length=False, testdata_type='dast'):
    pheme_ts, pheme_nts, dast_ts, dast_nts = load_datasets(unverified_cast)
    ts_data, nts_data = format_data(pheme_ts, pheme_nts, dast_ts, dast_nts, testdata_type)

    if include_branch_length:
        ts_performance = evaluate_for_splits_length(ts_data, unverified_cast, testdata_type)
        nts_performance = evaluate_for_splits_length(nts_data, unverified_cast, testdata_type)
    else:
        ts_performance = evaluate_for_splits_dataset_test(ts_data, unverified_cast, testdata_type)
        nts_performance = evaluate_for_splits_dataset_test(nts_data, unverified_cast, testdata_type)

    write_out(include_branch_length, ts_performance, nts_performance)


evaluate_performance('true', include_branch_length=False, testdata_type='dast')
