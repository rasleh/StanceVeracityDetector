import os
import sys

from src.data_loader import load_veracity
from src.models import hmm_veracity
import random
import copy

current_path = os.path.abspath(__file__)
pheme_data_path = os.path.join(current_path, '../../../data/datasets/pheme/preprocessed/veracity/')
dast_data_path = os.path.join(current_path, '../../../data/datasets/dast/preprocessed/veracity/')


class HiddenPrints:
    """Class for suppressing the abundant print statements from hmm_veracity.test, courtesy of
    https://stackoverflow.com/questions/8391411/suppress-calls-to-print-python"""
    def __enter__(self):
        self._original_stdout = sys.stdout
        sys.stdout = open(os.devnull, 'w')

    def __exit__(self, exc_type, exc_val, exc_tb):
        sys.stdout.close()
        sys.stdout = self._original_stdout


def cross_validation_splits(dataset, no_splits):
    """Generate an array of dataset splits, each index containing a tuple with train data at 0 and test data at 1. Takes
    preprocessed veracity data, as generated by data_loader.load_veracity() as input.

    :param dataset: an array which, at each index, contains a tuple, with the veracity of a claim at 0 and an array
    containing the features at index 1
    :param no_splits: The number of partitions the dataset should be split into
    :return: an array of dimensions (no_splits x 2 (representing train and test data) x len(train) or len(test)
    """
    splits = []

    # A few data-points will be lost due to rounding, 3 for DAST
    splitsize = int(len(dataset)/no_splits)
    last_loaded_index = 0
    for i in range(no_splits):
        test_data = dataset[last_loaded_index:last_loaded_index+splitsize]
        train_data = dataset[:last_loaded_index]
        train_data.extend(dataset[last_loaded_index+splitsize:])
        splits.append((train_data, test_data))
        last_loaded_index += splitsize
    return splits


def load_datasets(unverified_cast):
    """Loads the relevant datasets used for testing of veracity performance"""
    pheme_ts = load_veracity(os.path.join(pheme_data_path, 'timestamps.csv'), unverified_cast)
    pheme_nts = load_veracity(os.path.join(pheme_data_path, 'no_timestamps.csv'), unverified_cast)

    full_dast_ts = load_veracity(os.path.join(dast_data_path, 'timestamps.csv'), unverified_cast)
    full_dast_nts = load_veracity(os.path.join(dast_data_path, 'no_timestamps.csv'), unverified_cast)

    random.shuffle(full_dast_ts)
    random.shuffle(full_dast_nts)

    dast_ts_splits = cross_validation_splits(full_dast_ts, 5)
    dast_nts_splits = cross_validation_splits(full_dast_nts, 5)
    return pheme_ts, pheme_nts, dast_ts_splits, dast_nts_splits


def update_metrics_dataset(performance, model, acc, f1):
    performance[model]['f1_macro'] += f1
    performance[model]['accuracy'] += acc


def evaluate_for_splits_dataset(pheme_data, dast_data, unverified_cast):
    pheme_model = hmm_veracity.HMM(2)
    pheme_model.fit(pheme_data)

    performance = {
        'dast': {'f1_macro': 0.0, 'accuracy': 0.0},
        'pheme': {'f1_macro': 0.0, 'accuracy': 0.0},
        'dastpheme': {'f1_macro': 0.0, 'accuracy': 0.0}
    }

    for split in dast_data:
        _, acc, f1_macro, _ = pheme_model.test(split[1], unverified_cast)
        update_metrics_dataset(performance, 'pheme', acc, f1_macro)

        dastpheme_model = hmm_veracity.HMM(2)
        dastpheme_model.fit(pheme_data + split[0])
        _, acc, f1_macro, _ = dastpheme_model.test(split[1], unverified_cast)
        update_metrics_dataset(performance, 'dastpheme', acc, f1_macro)

        dast_model = hmm_veracity.HMM(2)
        dast_model.fit(split[0])
        _, acc, f1_macro, _ = dast_model.test(split[1], unverified_cast)
        update_metrics_dataset(performance, 'dast', acc, f1_macro)

    for dataset, results in performance.items():
        for metric, value in results.items():
            performance[dataset][metric] = value / len(dast_data)

    return performance


def evaluate_dataset_performance(unverified_cast):
    pheme_ts, pheme_nts, dast_ts_splits, dast_nts_splits = load_datasets(unverified_cast)

    # Hide abundance of print statements for hmm_veracity.test()
    with HiddenPrints():
        ts_performance = evaluate_for_splits_dataset(pheme_ts, dast_ts_splits, unverified_cast)
        nts_performance = evaluate_for_splits_dataset(pheme_nts, dast_nts_splits, unverified_cast)

    with open('veracity.csv', mode='w', encoding='utf-8') as out_file:
        out_file.write('model;f1_macro;accuracy\n')
        for dataset, results in ts_performance.items():
            out_file.write('{};{:.2f};{:.2f}\n'.format(dataset+'_ts', results['f1_macro'], results['accuracy']))

        for dataset, results in nts_performance.items():
            out_file.write('{};{:.2f};{:.2f}\n'.format(dataset, results['f1_macro'], results['accuracy']))


def update_metrics_length(performance, model, length, acc, f1):
    performance[model][length]['f1_macro'] += f1
    performance[model][length]['accuracy'] += acc


def evaluate_for_splits_length(pheme_data, dast_data, unverified_cast):
    pheme_model = hmm_veracity.HMM(2)
    pheme_model.fit(pheme_data)

    performance = {
        'dast': {
            1: {'f1_macro': 0.0, 'accuracy': 0.0},
            2: {'f1_macro': 0.0, 'accuracy': 0.0},
            3: {'f1_macro': 0.0, 'accuracy': 0.0},
            4: {'f1_macro': 0.0, 'accuracy': 0.0}
        },
        'pheme': {
            1: {'f1_macro': 0.0, 'accuracy': 0.0},
            2: {'f1_macro': 0.0, 'accuracy': 0.0},
            3: {'f1_macro': 0.0, 'accuracy': 0.0},
            4: {'f1_macro': 0.0, 'accuracy': 0.0}
        },
        'dastpheme': {
            1: {'f1_macro': 0.0, 'accuracy': 0.0},
            2: {'f1_macro': 0.0, 'accuracy': 0.0},
            3: {'f1_macro': 0.0, 'accuracy': 0.0},
            4: {'f1_macro': 0.0, 'accuracy': 0.0}
        },
    }


    for split in dast_data:
        length_seperated_data = {1: [], 2: [], 3: [], 4: []}
        for branch in split[0]:
            if len(branch[1]) not in length_seperated_data:
                length_seperated_data[4].append(branch)
            else:
                length_seperated_data[len(branch[1])].append(branch)

        dastpheme_model = hmm_veracity.HMM(2)
        dastpheme_model.fit(pheme_data + split[0])

        dast_model = hmm_veracity.HMM(2)
        dast_model.fit(split[0])

        for length, data in length_seperated_data.items():
            if data:
                _, acc, f1_macro, _ = pheme_model.test(data, unverified_cast)
                update_metrics_length(performance, 'pheme', length, acc, f1_macro)

                _, acc, f1_macro, _ = dastpheme_model.test(data, unverified_cast)
                update_metrics_length(performance, 'dastpheme', length, acc, f1_macro)

                _, acc, f1_macro, _ = dast_model.test(data, unverified_cast)
                update_metrics_length(performance, 'dast', length, acc, f1_macro)

    for dataset, results in performance.items():
        for length, metrics in results.items():
            for metric, value in metrics.items():
                performance[dataset][length][metric] = value / len(dast_data)

    return performance


def evaluate_thread_length_performance(unverified_cast):
    pheme_ts, pheme_nts, dast_ts_splits, dast_nts_splits = load_datasets(unverified_cast)

    with HiddenPrints():
        ts_performance = evaluate_for_splits_length(pheme_ts, dast_ts_splits, unverified_cast)
        nts_performance = evaluate_for_splits_length(pheme_nts, dast_nts_splits, unverified_cast)

    with open('veracity.csv', mode='w', encoding='utf-8') as out_file:
        out_file.write('model;length;f1_macro;accuracy\n')
        for dataset, results in ts_performance.items():
            for length, metrics in results.items():
                out_file.write('{};{};{:.2f};{:.2f}\n'.format(dataset+'_ts', length, results[length]['f1_macro'], results[length]['accuracy']))

        for dataset, results in nts_performance.items():
            for length, metrics in results.items():
                out_file.write('{};{};{:.2f};{:.2f}\n'.format(dataset + '_nts', length, results[length]['f1_macro'],
                                                           results[length]['accuracy']))


evaluate_dataset_performance('true')
